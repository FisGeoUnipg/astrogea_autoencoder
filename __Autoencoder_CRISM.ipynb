{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6692f156-6068-4655-9944-e2ba8d180cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('TkAgg')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from spectral import *\n",
    "import spectral.io.envi as envi\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from scipy.signal import savgol_filter, argrelextrema\n",
    "from matplotlib.widgets import *\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.backend_bases import MouseButton\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import rasterio\n",
    "from rasterio.control import GroundControlPoint\n",
    "from rasterio.transform import from_gcps\n",
    "from rasterio.crs import CRS\n",
    "from scipy.signal import savgol_filter , argrelextrema , find_peaks\n",
    "from kneed import KneeLocator\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator)\n",
    "from matplotlib.widgets import PolygonSelector\n",
    "from itertools import permutations \n",
    "import pandas as pd\n",
    "from scipy.spatial import ConvexHull\n",
    "from scipy.interpolate import interp1d\n",
    "from numpy.random import randint , uniform , choice\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader ,SubsetRandomSampler, random_split\n",
    "from torch import optim\n",
    "import itertools\n",
    "from sklearn.metrics import silhouette_score\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "29d82054-4cb2-4f08-94a3-c0d5eea88d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyfresco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9ac52e23-46cc-4ac5-9a62-baec34ff36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_wise_integral_norm_data(wav , spectra):\n",
    "\n",
    "    SPECTRA = np.zeros_like(spectra)\n",
    "\n",
    "    for i in range(len(spectra[:,0])):\n",
    "        SPECTRA[i] = spectra[i]/np.trapz(spectra[i] , wav)\n",
    "        \n",
    "    return SPECTRA , wav\n",
    "\n",
    "def column_wise_norm(spectra):\n",
    "    spectra_norm = np.zeros_like(spectra)\n",
    "    for i in range(len(spectra[0])):\n",
    "        spectra_norm[:,i] = (spectra[:,i]-np.mean(spectra[:,i]))/np.std(spectra[:,i])\n",
    "        \n",
    "    return spectra_norm\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def compute_removal_single(w , spectra , interp_type = 'linear'):\n",
    "    points = np.c_[w,spectra]\n",
    "    return continuum_removal(points , interp_type = interp_type)\n",
    "\n",
    "def dimension_reduction( img , w1 , w2 , wavelength , cr = False ):\n",
    "\n",
    "    X = img.shape[0]\n",
    "    Y = img.shape[1]\n",
    "    \n",
    "    a = find_nearest(wavelength , w1)\n",
    "    b = find_nearest(wavelength , w2)\n",
    "    w = wavelength[a:b]\n",
    "    lenw = len(w)\n",
    "    \n",
    "    A = np.count_nonzero(img[:,:,0] == 65535.)\n",
    "    B = np.count_nonzero(img[:,:,0] != 65535.)\n",
    "    \n",
    "    spectra = np.zeros( (B , lenw) )\n",
    "    indexes = np.zeros( (B , 2) )\n",
    "    \n",
    "    frame_indexes = np.zeros( (A , 2) )\n",
    " \n",
    "    img = img[:,:,a:b]\n",
    "    \n",
    "    k , i , j = 0 , 0 , 0\n",
    "    while k < B and i < X and j < Y:\n",
    "        \n",
    "        if img[i,j,0] != 65535. and img[i,j,:].all() != np.zeros(lenw).all():\n",
    "            if cr == False:\n",
    "                spectra[k,:] = img[i,j,:]\n",
    "            else:\n",
    "                spectra[k,:] = compute_removal_single(w , img[i,j,:])\n",
    "            indexes[k,0] = i\n",
    "            indexes[k,1] = j   \n",
    "            k += 1\n",
    "        if img[i,j,:].all() == np.zeros(lenw).all():\n",
    "            print(i,j)\n",
    "        i += 1\n",
    "        if i == X:\n",
    "            j += 1\n",
    "            i = 0\n",
    "            if j == Y:\n",
    "                i = X\n",
    "                j = Y\n",
    "                k = B\n",
    "                  \n",
    "    return spectra , indexes , w\n",
    "\n",
    "def dimension_reduction_spectral_parameters( img , norm = None , zeros = None ):\n",
    "\n",
    "    X = img.shape[0]\n",
    "    Y = img.shape[1]\n",
    "    lenw = img.shape[2]\n",
    "    print(X,Y,lenw)\n",
    "    B = np.count_nonzero(img[:,:,0] != 65535.)\n",
    "    print(B)\n",
    "    \n",
    "    spectra = np.zeros( (B , lenw) )\n",
    "    indexes = np.zeros( (B , 2) )\n",
    "    \n",
    "    k , i , j = 0 , 0 , 0\n",
    "    while k < B and i < X and j < Y:\n",
    "        \n",
    "        if img[i,j,0] != 65535.:\n",
    "            spectra[k,:] = img[i,j,:]#[0][0]\n",
    "            \n",
    "            if zeros != None:\n",
    "                for l in range(len(spectra[k,:])):\n",
    "                    if spectra[k,l] < 0:\n",
    "                        spectra[k,l] = 0\n",
    "            \n",
    "            indexes[k,0] = i\n",
    "            indexes[k,1] = j   \n",
    "            k += 1\n",
    "        i += 1\n",
    "        if i == X:\n",
    "            j += 1\n",
    "            i = 0\n",
    "            if j == Y:\n",
    "                i = X\n",
    "                j = Y\n",
    "                k = B\n",
    "\n",
    "    if norm == 'none':\n",
    "        spectra = np.nan_to_num(spectra)\n",
    "        return spectra, indexes\n",
    "        \n",
    "    elif len(norm) > 0:\n",
    "        for p in range(len(norm)):\n",
    "    \n",
    "            if p != 0:\n",
    "                spectra = SPECTRA\n",
    "            SPECTRA = np.zeros_like(spectra)\n",
    "            \n",
    "            if norm[p] == 'row':\n",
    "                for i in range(len(spectra[:,0])):\n",
    "                    SPECTRA[i] = spectra[i]/np.sum(abs(spectra[i]))\n",
    "    \n",
    "            elif norm[p] == 'column':\n",
    "                for i in range(len(spectra[0])):\n",
    "                    if zeros != None:\n",
    "                        SPECTRA[:,i] = (spectra[:,i]-np.mean(spectra[:,i]))/np.std(spectra[:,i])\n",
    "                    else:\n",
    "                        m , s = np.mean(spectra[ np.argwhere(spectra[:,i] != 0) , i ]) , np.std(spectra[ np.argwhere(spectra[:,i] != 0) , i ])\n",
    "                        SPECTRA[:,i] = (spectra[:,i]-m)/s\n",
    "    \n",
    "            elif norm[p] == 'minmax':\n",
    "                for i in range(len(spectra[0])):\n",
    "                    if zeros != None:\n",
    "                        SPECTRA[:,i] = (spectra[:,i]-np.min(spectra[:,i]))/(np.max(spectra[:,i]) - np.min(spectra[:,i]))\n",
    "    \n",
    "            elif norm[p] == 'L1':\n",
    "                for i in range(len(spectra[0])):\n",
    "                    if zeros != None:\n",
    "                        SPECTRA[:,i] = spectra[:,i]/np.linalg.norm(spectra[:,i])\n",
    "                        \n",
    "            SPECTRA = np.nan_to_num(SPECTRA)\n",
    "\n",
    "        return SPECTRA , indexes\n",
    "\n",
    "    else:\n",
    "        print('Normalization must be either row, column, rowcolumn or none!')\n",
    "        return\n",
    "\n",
    "def continuum_removal(points , interp_type = 'linear'):#points, interp_type = 'linear'):\n",
    "    interp_types = ['linear','nearest','nearest-up','zero','slinear','quadratic','cubic','previous','next']\n",
    "    \n",
    "    if interp_type not in interp_types:\n",
    "        print('Type of interpolation must be one of:' + interp_types + '.')\n",
    "        return\n",
    "    \n",
    "    x, y = points.T\n",
    "    augmented = np.concatenate([points, [(x[0], np.min(y)-1), (x[-1], np.min(y)-1)]], axis=0)\n",
    "    hull = ConvexHull(augmented , incremental = True)\n",
    "    continuum_points = points[np.sort([v for v in hull.vertices if v < len(points)])]\n",
    "    continuum_indexs = np.array(range(0,len(continuum_points) , 1) , dtype = int)\n",
    "    continuum_function = interp1d(*continuum_points.T)\n",
    "    n = continuum_function(x)\n",
    "    \n",
    "    yprime = y / n\n",
    "\n",
    "    return yprime\n",
    "\n",
    "def compute_removal(w , spectra , interp_type = 'linear'):\n",
    "    SPECTRA = np.zeros(spectra.shape)\n",
    "    for i in range(spectra.shape[0]):\n",
    "        points = np.c_[w,spectra[i]]\n",
    "        SPECTRA[i] = continuum_removal(points , interp_type = interp_type)#points)\n",
    "    return SPECTRA\n",
    "\n",
    "def compute_removal_single(w , spectra , interp_type = 'linear'):\n",
    "    points = np.c_[w,spectra]\n",
    "    return continuum_removal(points , interp_type = interp_type)\n",
    "\n",
    "def auto_stretch_rgb(img_sr, product_names, n_bins=1000, plot=True):\n",
    "    local_only_products = { 'R530' , 'R440' , 'R600' , 'R770' , 'R1080' , 'R1506' , 'R2529' , 'R3920' , 'SH600_2' , 'IRA' , 'ISLOPE1' , 'IRR2' }\n",
    "\n",
    "    H, W, num_bands = img_sr.shape\n",
    "    stretched_cube = np.zeros((H, W, num_bands), dtype=np.float32)\n",
    "\n",
    "    for band_idx in range(num_bands):\n",
    "        band = img_sr[:, :, band_idx]\n",
    "        name = product_names[band_idx]\n",
    "        valid = np.isfinite(band) & (band != 65535)\n",
    "        vals = band[valid]\n",
    "\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "\n",
    "        if name in local_only_products:\n",
    "            vmin, vmax = np.percentile(vals, [0.1, 99.9])\n",
    "        else:\n",
    "            hist, bins = np.histogram(vals, bins=n_bins)\n",
    "            mode = (bins[np.argmax(hist)] + bins[np.argmax(hist)+1]) / 2\n",
    "            vmin = 0 if mode < 0 else mode\n",
    "            vmax = np.percentile(vals, 99.9)\n",
    "\n",
    "        stretched = np.zeros_like(band, dtype=np.float32)\n",
    "        stretched[valid] = np.clip((band[valid] - vmin) / (vmax - vmin), 0, 1)\n",
    "        stretched_cube[:, :, band_idx] = stretched[:,:,0]\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "            ax[0].hist(vals, bins=n_bins, color='gray', log=True)\n",
    "            ax[0].axvline(vmin, color='blue', linestyle='--', label='vmin')\n",
    "            ax[0].axvline(vmax, color='red', linestyle='--', label='vmax')\n",
    "            ax[0].set_title(f\"{name} Histogram\")\n",
    "            ax[0].legend()\n",
    "            ax[0].set_xlim(np.percentile(vals, 0), np.percentile(vals, 100))\n",
    "\n",
    "            ax[1].imshow(stretched, cmap='gray', vmin=0, vmax=1)\n",
    "            ax[1].set_title(f\"{name} Stretched\")\n",
    "            ax[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    return stretched_cube\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def auto_stretch_rgb(img_sr, product_names, n_bins=1000, plot=True, linearize=False, norm=None, zeros=False):\n",
    "    local_only_products = {\n",
    "        'R530', 'R440', 'R600', 'R770', 'R1080', 'R1506',\n",
    "        'R2529', 'R3920', 'SH600_2', 'IRA', 'ISLOPE1', 'IRR2'\n",
    "    }\n",
    "\n",
    "    H, W, num_bands = img_sr.shape\n",
    "    stretched_cube = np.zeros((H, W, num_bands), dtype=np.float32)\n",
    "\n",
    "    for band_idx in range(num_bands):\n",
    "        band = img_sr[:, :, band_idx]\n",
    "        name = product_names[band_idx]\n",
    "        valid = np.isfinite(band) & (band != 65535)\n",
    "        vals = band[valid]\n",
    "\n",
    "        if vals.size == 0:\n",
    "            continue\n",
    "\n",
    "        if name in local_only_products:\n",
    "            vmin, vmax = np.percentile(vals, [0.1, 99.9])\n",
    "        else:\n",
    "            hist, bins = np.histogram(vals, bins=n_bins)\n",
    "            mode = (bins[np.argmax(hist)] + bins[np.argmax(hist)+1]) / 2\n",
    "            vmin = 0 if mode < 0 else mode\n",
    "            vmax = np.percentile(vals, 99.9)\n",
    "\n",
    "        stretched = np.zeros_like(band, dtype=np.float32)\n",
    "        stretched[valid] = np.clip((band[valid] - vmin) / (vmax - vmin), 0, 1)\n",
    "        stretched_cube[:, :, band_idx] = stretched#[:,:,0]\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "            ax[0].hist(vals, bins=n_bins, color='gray', log=True)\n",
    "            ax[0].axvline(vmin, color='blue', linestyle='--', label='vmin')\n",
    "            ax[0].axvline(vmax, color='red', linestyle='--', label='vmax')\n",
    "            ax[0].set_title(f\"{name} Histogram\")\n",
    "            ax[0].legend()\n",
    "            ax[0].set_xlim(np.percentile(vals, 0.5), np.percentile(vals, 99.9))\n",
    "\n",
    "            ax[1].imshow(stretched, cmap='gray', vmin=0, vmax=1)\n",
    "            ax[1].set_title(f\"{name} Stretched\")\n",
    "            ax[1].axis('off')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "    if not linearize:\n",
    "        return stretched_cube\n",
    "\n",
    "    # LINEARIZATION\n",
    "    valid_mask = img_sr[:, :, 0] != 65535\n",
    "    indexes = np.argwhere(valid_mask)\n",
    "    spectra = stretched_cube[valid_mask]  # shape: (N_valid, num_bands)\n",
    "\n",
    "    if zeros:\n",
    "        spectra[spectra < 0] = 0\n",
    "\n",
    "    if norm is None or norm == 'none':\n",
    "        return spectra, indexes\n",
    "\n",
    "    # Apply normalization chain\n",
    "    SPECTRA = spectra.copy()\n",
    "\n",
    "    for n in norm:\n",
    "        if n == 'row':\n",
    "            row_sums = np.sum(np.abs(SPECTRA), axis=1, keepdims=True)\n",
    "            row_sums[row_sums == 0] = 1\n",
    "            SPECTRA = SPECTRA / row_sums\n",
    "\n",
    "        elif n == 'column':\n",
    "            means = np.mean(SPECTRA, axis=0)\n",
    "            stds = np.std(SPECTRA, axis=0)\n",
    "            stds[stds == 0] = 1\n",
    "            SPECTRA = (SPECTRA - means) / stds\n",
    "\n",
    "        elif n == 'minmax':\n",
    "            mins = np.min(SPECTRA, axis=0)\n",
    "            maxs = np.max(SPECTRA, axis=0)\n",
    "            denom = maxs - mins\n",
    "            denom[denom == 0] = 1\n",
    "            SPECTRA = (SPECTRA - mins) / denom\n",
    "\n",
    "        elif n == 'L1':\n",
    "            norms = np.linalg.norm(SPECTRA, ord=1, axis=0)\n",
    "            norms[norms == 0] = 1\n",
    "            SPECTRA = SPECTRA / norms\n",
    "\n",
    "        else:\n",
    "            print(f\"Unknown norm: {n}\")\n",
    "\n",
    "    SPECTRA = np.nan_to_num(SPECTRA)\n",
    "\n",
    "    return SPECTRA, indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e582fcef-64b9-4ba2-9b10-50c9c13942aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FRT00009b5a/frt00009b5a_07_if165j_mtr3.img\n",
      "['R770', 'RBR', 'BD530_2', 'SH600_2', 'SH770', 'BD640_2', 'BD860_2', 'BD920_2', 'RPEAK1', 'BDI1000VIS', 'R440', 'IRR1', 'BDI1000IR', 'OLINDEX3', 'R1330', 'BD1300', 'LCPINDEX2', 'HCPINDEX2', 'VAR', 'ISLOPE1', 'BD1400', 'BD1435', 'BD1500_2', 'ICER1_2', 'BD1750_2', 'BD1900_2', 'BD1900R2', 'BDI2000', 'BD2100_2', 'BD2165', 'BD2190', 'MIN2200', 'BD2210_2', 'D2200', 'BD2230', 'BD2250', 'MIN2250', 'BD2265', 'BD2290', 'D2300', 'BD2355', 'SINDEX2', 'ICER2_2', 'MIN2295_2480', 'MIN2345_2537', 'BD2500_2', 'BD3000', 'BD3100', 'BD3200', 'BD3400_2', 'CINDEX2', 'BD2600', 'IRR2', 'IRR3', 'R530', 'R600', 'R1080', 'R1506', 'R2529', 'R3920']\n",
      "['BD530_2' 'SH600_2' 'SH770' 'BD640_2' 'BD860_2' 'BD920_2' 'BDI1000VIS'\n",
      " 'BDI1000IR' 'OLINDEX3' 'BD1300' 'LCPINDEX2' 'HCPINDEX2' 'VAR' 'ISLOPE1'\n",
      " 'BD1400' 'BD1435' 'BD1500_2' 'ICER1_2' 'BD1750_2' 'BD1900_2' 'BD1900R2'\n",
      " 'BDI2000' 'BD2100_2' 'BD2165' 'BD2190' 'MIN2200' 'BD2210_2' 'D2200'\n",
      " 'BD2230' 'BD2250' 'MIN2250' 'BD2265' 'BD2290' 'D2300' 'BD2355' 'SINDEX2'\n",
      " 'ICER2_2' 'MIN2295_2480' 'MIN2345_2537' 'BD2500_2' 'BD3000' 'BD3100'\n",
      " 'BD3200' 'BD3400_2' 'CINDEX2' 'BD2600']\n",
      "(744, 781, 60)\n"
     ]
    }
   ],
   "source": [
    "path09b5a = \"FRT00009b5a/\"#\"OneDrive/Desktop/cartOrder_part_0002/cartorder/\"\n",
    "path120e7 = \"../../Desktop/Thyrrenus/frt000120e7/\"\n",
    "path17cd0 = \"../../Desktop/Thyrrenus/frt00017cd0/\"\n",
    "path3e24 = \"Desktop/FRT00003E24_07_IF165J_MTR3/\"\n",
    "path62e6 = \"Desktop/FRT000062e6/\"\n",
    "path3e12 = \"Desktop/FRT00003e12/\"\n",
    "path = path09b5a#120e7\n",
    "name = ['09b5a' , '120e7' , '17cd0' , '03e24' , '062e6' , '05d88' , '067b7' , '07a2c' , '1487f' , '07d63' , '0660f' , '03e12']\n",
    "fof = ['5' , '4' , '4' , '5' , '5' , '3' , '3' , '3' , '3' , '3' , '3' , '6']\n",
    "i = np.argwhere(np.array(name) == '09b5a')[0][0]#len(fof)-1\n",
    "\n",
    "# Example usage:\n",
    "path_if_mtrdr = path + \"frt000\" + name[i] + \"_07_if16\" + fof[i] + \"j_mtr3.img\"\n",
    "head_if_mtrdr = path + \"frt000\" + name[i] + \"_07_if16\" + fof[i] + \"j_mtr3.hdr\"\n",
    "\n",
    "path_sr_mtrdr = path + \"frt000\" + name[i] + \"_07_sr16\" + fof[i] + \"j_mtr3.img\"\n",
    "head_sr_mtrdr = path + \"frt000\" + name[i] + \"_07_sr16\" + fof[i] + \"j_mtr3.hdr\"\n",
    "\n",
    "print(path_if_mtrdr)\n",
    "\n",
    "img , img_sr , wavelength , sr_names = pyfresco.RGBmap.open_raw(path_if_mtrdr , head_if_mtrdr , path_sr_mtrdr , head_sr_mtrdr)  # Provide your image data\n",
    "\n",
    "Nbands = len(wavelength)\n",
    "print(sr_names)\n",
    "\n",
    "img , img_sr = np.array(img[:,:,:]) , np.array(img_sr[:,:,:])\n",
    "\n",
    "sr_names_2 = np.delete(np.array(sr_names) ,\n",
    "                       [[0],[1],[8],[10],[11],[14],[52],[53],[54],[55],[56],[57],[58],[59]])\n",
    "print(sr_names_2)\n",
    "\n",
    "img_sr_2 = np.delete(img_sr[:,:,:] , [[0],[1],[8],[10],[11],[14],[52],[53],[54],[55],[56],[57],[58],[59]] , axis = 2)\n",
    "\n",
    "print(img_sr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8b465b50-2add-47cc-86da-a1e7f3da006c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "def process_pixel(i, j, img, x, wavelength, limI, limU, limA, limB, force, interp):\n",
    "    spectrum = img[i, j, :]\n",
    "    if spectrum[0] == 65535:\n",
    "        return None\n",
    "\n",
    "    y = spectrum[limI:limU]\n",
    "    points = np.c_[x, y]\n",
    "    augmented = np.concatenate([points, [(x[0], np.min(y)-1), (x[-1], np.min(y)-1)]])\n",
    "    hull = ConvexHull(augmented)\n",
    "    continuum_points = points[np.sort([v for v in hull.vertices if v < len(points)])]\n",
    "\n",
    "    if force:\n",
    "        sub_x = wavelength[limA:limB]\n",
    "        sub_y = spectrum[limA:limB]\n",
    "        if len(sub_y) == 0:\n",
    "            return None\n",
    "        max_idx = np.argmax(sub_y)\n",
    "        max_wave = sub_x[max_idx]\n",
    "        max_val = sub_y[max_idx]\n",
    "        continuum_points = np.vstack([continuum_points, [max_wave, max_val]])\n",
    "        continuum_points = continuum_points[np.argsort(continuum_points[:, 0])]\n",
    "\n",
    "    f = interp1d(*continuum_points.T, kind=interp, fill_value=\"extrapolate\")\n",
    "    neutral = f(x)\n",
    "    return y / neutral\n",
    "    \n",
    "def continuum_removal_parallel(img, wavelength, MIN, MAX, interp='linear', force=False, forcemin=1500, forcemax=1800):\n",
    "    I, J, K = img.shape\n",
    "    limI = int(np.argmin(np.abs(wavelength - MIN)))\n",
    "    limU = int(np.argmin(np.abs(wavelength - MAX)))\n",
    "    x = wavelength[limI:limU]\n",
    "    limA = int(np.argmin(np.abs(wavelength - forcemin)))\n",
    "    limB = int(np.argmin(np.abs(wavelength - forcemax)))\n",
    "\n",
    "    result = np.zeros((I, J, len(x)))\n",
    "\n",
    "    parallel = Parallel(n_jobs=8 , backend=\"loky\")\n",
    "    results = parallel(\n",
    "        delayed(process_pixel)(i, j, img, x, wavelength, limI, limU, limA, limB, force, interp)\n",
    "        for i in range(I)\n",
    "        for j in range(J)\n",
    "    )\n",
    "\n",
    "    k = 0\n",
    "    for i in range(I):\n",
    "        for j in range(J):\n",
    "            res = results[k]\n",
    "            if res is not None:\n",
    "                result[i, j, :] = res\n",
    "            k += 1\n",
    "\n",
    "    return result, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "1520eb65-0bbf-447c-be2b-2cc582cd8b3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "Done\n",
      "(394890, 277)\n"
     ]
    }
   ],
   "source": [
    "def unison_shuffled_copies(a, b , SEED = 311996):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.RandomState(seed=SEED).permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "#spectra , indexes = dimension_reduction_spectral_parameters( img_sr , norm = ['L1'] , zeros = True )# , 500 , 2500 , wavelength)\n",
    "#spectra , indexes = auto_stretch_rgb(img_sr_2, sr_names_2, n_bins=1000, plot=False, linearize=True, norm=['column'], zeros=True)\n",
    "#print( 'reduction done' , spectra.shape )\n",
    "\n",
    "\n",
    "# try with the spectra\n",
    "a = find_nearest(wavelength , 750)\n",
    "b = find_nearest(wavelength , 2600)\n",
    "img_cr, wav2 = continuum_removal_parallel(img, wavelength, MIN = 750, MAX=2600)\n",
    "#spectra = compute_removal(wavelength[a:b] , spectra2 , interp_type = 'linear')\n",
    "#print('continuum removal done' , spectra.shape)\n",
    "#spectra , w = row_wise_integral_norm_data(w , spectra)\n",
    "#print('row normalization done' , spectra.shape)\n",
    "#spectra = column_wise_norm(spectra)\n",
    "#print('column normalization done' , spectra.shape)\n",
    "\n",
    "#spectra2 , indexes2 , wav = dimension_reduction( img , 400 , 2600 , wavelength , cr = True )\n",
    "\n",
    "#spectra , indexes = unison_shuffled_copies(spectra , indexes)\n",
    "#spectra2 , indexes2 = unison_shuffled_copies(spectra2 , indexes2)\n",
    "\n",
    "print( np.count_nonzero(spectra.all() == np.zeros(img.shape[2]).all()) )\n",
    "print( 'Done' )\n",
    "\n",
    "print(spectra.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4240474d-e594-415a-8989-197afe4a9f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37 314\n"
     ]
    }
   ],
   "source": [
    "print(a , b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "64308469-ce0c-45ef-9c45-68a6b8a68c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra = np.zeros((np.count_nonzero(img[:,:,0] != 65535.) , len(wav2)))\n",
    "indexes = np.zeros( (np.count_nonzero(img[:,:,0] != 65535.) , 2) )\n",
    "k = 0\n",
    "for i in range(img.shape[0]):\n",
    "    for j in range(img.shape[1]):\n",
    "        if img[i,j,0] != 65535:\n",
    "            spectra[k , :] = img_cr[i,j,:]\n",
    "            indexes[k , 0] = i\n",
    "            indexes[k , 1] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d30cc16-3f3f-4d9b-9b7a-c5bba0d7a473",
   "metadata": {},
   "outputs": [],
   "source": [
    "rgbs = auto_stretch_rgb(img_sr, sr_names, n_bins=1000, plot=False, linearize=False, norm=None, zeros=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "939f86cf-34f2-4b4f-be78-036b8578445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(10, 6, sharex=True, sharey=True, figsize=(12, 20))\n",
    "\n",
    "k = 0\n",
    "im = None\n",
    "for i in range(10):\n",
    "    for j in range(6):\n",
    "        im = ax[i, j].imshow(rgbs[:, :, k])  # save the imshow object\n",
    "        ax[i, j].set_title(sr_names[k], fontsize=8, y=0.875)\n",
    "        ax[i, j].axis('off')\n",
    "        k += 1\n",
    "\n",
    "# Add one common colorbar to the left\n",
    "cbar = fig.colorbar(im, ax=ax.ravel().tolist(), orientation=\"vertical\", location=\"left\")\n",
    "cbar.set_label(\"Spectral Parameter Normalized Value\")  # optional label\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1e50b925-e9e1-4d40-8e03-2b7744f15ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random search ok!\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, encoded_space_dim , in_channels , n_layers_encoder , n_layers_decoder , out1 , out2 , act , last):\n",
    "        self.encoded_space_dim = encoded_space_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.n_layers_encoder = n_layers_encoder\n",
    "        self.n_layers_decoder = n_layers_decoder\n",
    "        self.out1 = out1\n",
    "        self.out2 = out2\n",
    "    \n",
    "        #super(Net, self).__init__()\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = []\n",
    "        if self.n_layers_encoder == 1:\n",
    "            self.model.append(nn.Linear(self.in_channels , self.out1[0]))\n",
    "            self.model.append(act)\n",
    "            self.model.append(nn.Linear(self.out1[0] , encoded_space_dim))\n",
    "        \n",
    "        elif self.n_layers_encoder == 0:\n",
    "            self.model.append(nn.Linear(self.in_channels , encoded_space_dim))\n",
    "            self.model.append(act)\n",
    "        \n",
    "        else:\n",
    "            for i in range(self.n_layers_encoder):\n",
    "                if i == self.n_layers_encoder-1:\n",
    "                    self.model.append(nn.Linear(self.out1[i] , self.encoded_space_dim))\n",
    "                elif i == 0:\n",
    "                    self.model.append(nn.Linear(self.in_channels , self.out1[i]))\n",
    "                    self.model.append(act)\n",
    "                    self.model.append(nn.Linear(self.out1[i] , self.out1[i+1]))\n",
    "                    self.model.append(act)\n",
    "                else:   \n",
    "                    self.model.append(nn.Linear(self.out1[i] , self.out1[i+1]))\n",
    "                    self.model.append(act)\n",
    "        self.encoder = nn.Sequential(*self.model)\n",
    "        \n",
    "        self.model2 = []\n",
    "        \n",
    "        if self.n_layers_decoder == 0:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim , self.in_channels))\n",
    "        elif self.n_layers_decoder == 1:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim , self.out2[0]))\n",
    "            self.model2.append(act)\n",
    "            self.model2.append(nn.Linear(self.out2[0] , self.in_channels))\n",
    "        else:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim , self.out2[0]))\n",
    "            self.model2.append(act)\n",
    "            for i in range(self.n_layers_decoder-1):\n",
    "                self.model2.append(nn.Linear(self.out2[i] , self.out2[i+1]))\n",
    "                self.model2.append(act)\n",
    "            self.model2.append(nn.Linear(self.out2[-1] , self.in_channels))\n",
    "        \n",
    "        if last == True:\n",
    "            self.model2.append(nn.Softmax())\n",
    "\n",
    "        self.decoder = nn.Sequential(*self.model2)\n",
    "\n",
    "    def _init_weights(self, layer):\n",
    "        if isinstance(layer, nn.Linear):\n",
    "            nn.init.kaiming_uniform_(layer.weight, nonlinearity='relu')\n",
    "            if layer.bias is not None:\n",
    "                layer.bias.data.zero_()\n",
    "        self.apply(self._init_weights)\n",
    "                \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def train_cnn(model, criterion, train_ds, validation_ds, num_epochs, patience, weight_decay, bs, device, LR, printer = False):\n",
    "    model.to(device)\n",
    "    train_loader = DataLoader(train_ds, batch_size=1024, shuffle=False, pin_memory=True)\n",
    "    validation_loader = DataLoader(validation_ds, batch_size=1024, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=patience, factor=0.5)\n",
    "    train_losses = []\n",
    "    validation_losses = []\n",
    "    best_accuracy = 0\n",
    "    no_improvement = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x in train_loader:\n",
    "            x = x.float().to(device, non_blocking=True)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x)\n",
    "            loss = criterion(outputs, x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        train_loss /= len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            validation_loss = 0\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            for x in validation_loader:\n",
    "                x = x.float().to(device, non_blocking=True)\n",
    "                outputs = model(x)\n",
    "                loss = criterion(outputs, x)\n",
    "                validation_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                y_true.extend(x.data.detach().cpu().numpy())\n",
    "                y_pred.extend(predicted.detach().cpu().numpy())\n",
    "            validation_loss /= len(validation_loader)\n",
    "            validation_losses.append(validation_loss)\n",
    "            scheduler.step(validation_loss)\n",
    "            \n",
    "            if printer == True:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs} -> Train Loss: {train_loss:.4f} | Validation Loss: {validation_loss:.4f} ')\n",
    "            \n",
    "    print('Finished Training')\n",
    "    return train_losses, validation_losses#, accuracy, precision, recall, f1\n",
    "\n",
    "def randomized_search_with_kfold(data_set, device, in_channels , criterion ,\n",
    "                                 num_trials=10, k=5, param_space={},\n",
    "                                 patience = 10 ):    \n",
    "    \n",
    "    #labels, train_fn,\n",
    "    kf = KFold(n_splits=k, shuffle=True)\n",
    "    best_accuracy = 0\n",
    "    best_params = None\n",
    "    torch.manual_seed(1)\n",
    "    for i in range(num_trials):\n",
    "        print('Trial [{}/{}]'.format(i+1, num_trials))\n",
    "        # Randomly sample parameters from the parameter space\n",
    "        params = {key: random.choice(values) for key, values in param_space.items()}\n",
    "        \n",
    "        #if params['num_layers_encoder'] > 1:\n",
    "        params['out1'] = np.array(np.sort(params['out1'][:params['num_layers_encoder']]) , dtype = int)[::-1]\n",
    "            \n",
    "        #if params['num_layers_decoder'] > 1:\n",
    "        params['out2'] = np.array(np.sort(params['out2'][:params['num_layers_decoder']]) , dtype = int)\n",
    "        \n",
    "        print(params)\n",
    "            \n",
    "        num_conv_layers , num_linear_layers = params['num_layers_encoder'] , params['num_layers_decoder']\n",
    "        # Build the model\n",
    "        \n",
    "        model = Net(3 , in_channels, params['num_layers_encoder'] , params['num_layers_decoder'] ,\n",
    "                    params['out1'] , params['out2'] , params['act'] , False ).to(device)\n",
    "        #weight_init(model, init_method = params['weight_init'])\n",
    "        #model.to(device)\n",
    "        \n",
    "        fold = 0\n",
    "        for train_index, test_index in kf.split(shuffle(data_set)):\n",
    "            train_data, test_data = data_set[train_index,:], data_set[test_index,:]\n",
    "            train_losses, validation_losses = train_cnn(model, criterion, train_data, test_data, params['num_epochs'], \n",
    "                                                        patience, params['weight_decay'], params['batch_size'],\n",
    "                                                        device = device, LR = params['learning_rate'])\n",
    "            \n",
    "            print('Fold [{}/{}]'.format(fold+1, k), 'with validation loss = ', validation_losses[-1])\n",
    "            fold += 1\n",
    "            \n",
    "            if validation_losses[-1] > best_accuracy:\n",
    "                best_accuracy = validation_losses[-1]\n",
    "                best_params = params\n",
    "                \n",
    "        torch.cuda.empty_cache()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        print('---------------------------------------------------')\n",
    "    return best_accuracy, best_params\n",
    "print('random search ok!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8835eafd-7f58-417b-a55e-fe0df5fe30d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GumbelSoftmax(nn.Module):\n",
    "    def __init__(self, temperature=1.0, hard=False):\n",
    "        super(GumbelSoftmax, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.hard = hard\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.gumbel_softmax(x, tau=self.temperature, hard=self.hard)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoded_space_dim, in_channels, n_layers_encoder, n_layers_decoder, out1, out2, act, drops , last):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.encoded_space_dim = encoded_space_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.n_layers_encoder = n_layers_encoder\n",
    "        self.n_layers_decoder = n_layers_decoder\n",
    "        self.out1 = out1\n",
    "        self.out2 = out2\n",
    "        self.drops = drops\n",
    "    \n",
    "        self.model = []\n",
    "        if self.n_layers_encoder == 1:\n",
    "            self.model.append(nn.Linear(self.in_channels, self.out1[0]))\n",
    "            self.model.append(nn.BatchNorm1d(self.out1[0]))\n",
    "            self.model.append(act)\n",
    "            self.model.append(nn.Dropout(self.drops[0]))\n",
    "            self.model.append(nn.Linear(self.out1[0], encoded_space_dim))\n",
    "            \n",
    "        elif self.n_layers_encoder == 0:\n",
    "            self.model.append(nn.Linear(self.in_channels, encoded_space_dim))\n",
    "            self.model.append(act)\n",
    "        else:\n",
    "            for i in range(self.n_layers_encoder):\n",
    "                if i == self.n_layers_encoder-1:\n",
    "                    self.model.append(nn.Linear(self.out1[i], self.encoded_space_dim))\n",
    "                elif i == 0:\n",
    "                    self.model.append(nn.Linear(self.in_channels, self.out1[i]))\n",
    "                    self.model.append(nn.BatchNorm1d(self.out1[i]))\n",
    "                    self.model.append(nn.Dropout(self.drops[i]))\n",
    "                    self.model.append(act)\n",
    "                    self.model.append(nn.Linear(self.out1[i], self.out1[i+1]))\n",
    "                    self.model.append(nn.Dropout(self.drops[i+1]))\n",
    "                    self.model.append(act)\n",
    "                else:   \n",
    "                    self.model.append(nn.Linear(self.out1[i], self.out1[i+1]))\n",
    "                    self.model.append(nn.BatchNorm1d(self.out1[i+1]))\n",
    "                    self.model.append(nn.Dropout(self.drops[i+1]))\n",
    "                    self.model.append(act)\n",
    "                    \n",
    "        # Add GumbelSoftmax to nn.Sequential\n",
    "        self.encoder = nn.Sequential(*self.model, GumbelSoftmax(temperature=1.0, hard=True))\n",
    "        \n",
    "        self.model2 = []\n",
    "        \n",
    "        if self.n_layers_decoder == 0:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim, self.in_channels))\n",
    "        elif self.n_layers_decoder == 1:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim, self.out2[0]))\n",
    "            self.model2.append(act)\n",
    "            self.model2.append(nn.Linear(self.out2[0], self.in_channels))\n",
    "        else:\n",
    "            self.model2.append(nn.Linear(self.encoded_space_dim, self.out2[0]))\n",
    "            self.model2.append(act)\n",
    "            for i in range(self.n_layers_decoder-1):\n",
    "                self.model2.append(nn.Linear(self.out2[i], self.out2[i+1]))\n",
    "                self.model2.append(act)\n",
    "            self.model2.append(nn.Linear(self.out2[self.n_layers_decoder-1], self.in_channels))\n",
    "            self.model2.append(nn.Tanh())\n",
    "        self.decoder = nn.Sequential(*self.model2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return idx\n",
    "\n",
    "def weight_init(model, init_method):\n",
    "    for module in model.modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            if init_method == 'kaiming_normal':\n",
    "                nn.init.kaiming_normal_(module.weight)\n",
    "            elif init_method == 'kaiming_uniform':\n",
    "                nn.init.kaiming_uniform_(module.weight)\n",
    "            elif init_method == 'xavier_normal':\n",
    "                nn.init.xavier_normal_(module.weight)\n",
    "            elif init_method == 'xavier_uniform':\n",
    "                nn.init.xavier_uniform_(module.weight)\n",
    "            elif init_method == 'uniform':\n",
    "                nn.init.uniform_(module.weight)\n",
    "            elif init_method == 'normal':\n",
    "                nn.init.normal_(module.weight)\n",
    "            elif init_method == 'ones':\n",
    "                nn.init.ones_(module.weight)\n",
    "            #elif init_method == 'zeros':\n",
    "            #    nn.init.zeros_(module.weight)\n",
    "            elif init_method == 'eye':\n",
    "                nn.init.eye_(module.weight)\n",
    "            elif init_method == 'orthogonal':\n",
    "                nn.init.orthogonal_(module.weight)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid initialization method!\")\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "        elif isinstance(module, nn.BatchNorm1d):\n",
    "            nn.init.constant_(module.weight, 1)\n",
    "            nn.init.constant_(module.bias, 0)\n",
    "\n",
    "INITS = ['kaiming_normal' , 'kaiming_uniform' , 'xavier_normal' , 'xavier_uniform' , 'uniform' , 'normal' , 'ones' ,# 'zeros' ,\n",
    "         'eye' , 'orthogonal']\n",
    "def RandomSearch_autoencoder(dataset , in_channels, criterion , encoded_space_dim ,\n",
    "                             n_encmax , n_decmax , MINenc , MAXenc, MINdec, MAXdec,\n",
    "                             activations , initializations = INITS,\n",
    "                             fix_enc = False , fix_dec = False ,\n",
    "                             try_epochs = 10 , N_try = 10 , Seed = torch.seed() ,\n",
    "                             printer = 'off' , bs = 100 , num_pieces = 5 , val_split = 0.2):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(device)\n",
    "    \n",
    "    # Initialize names and arrays of hyperparameters\n",
    "    hyperparanames = ['LR','outC','outL','n_conv_layers','n_lin_layers','encoded_space_dim','weight_initialization']\n",
    "    \n",
    "    rate , W , L1 = [] , [] , []\n",
    "    train_l = []\n",
    "    val_l = []\n",
    "    Lenc = np.zeros((N_try , n_encmax))\n",
    "    Ldec = np.zeros((N_try , n_decmax))\n",
    "    enc = []\n",
    "    ACT = []\n",
    "    DROPS = []\n",
    "    inits = []\n",
    "\n",
    "    total_len = len(dataset)\n",
    "    val_len = int(total_len * val_split)\n",
    "    train_len = total_len - val_len\n",
    "    \n",
    "    train_set, val_set = random_split(dataset, [train_len, val_len], generator=torch.Generator().manual_seed(Seed))\n",
    "    TL = DataLoader(train_set, batch_size=bs, shuffle=True, pin_memory=True)\n",
    "    VL = DataLoader(val_set, batch_size=bs, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    for i in range(N_try):\n",
    "        \n",
    "        losses = []\n",
    "        validations = []\n",
    "        \n",
    "        ENCSPDIM = choice(encoded_space_dim)\n",
    "        \n",
    "        print('Try' , str(i))\n",
    "        \n",
    "        if fix_dec == False:\n",
    "            n_dec = randint( 0 , n_decmax+1 )\n",
    "        else:\n",
    "            n_dec = n_decmax\n",
    "            \n",
    "        if fix_enc == False:\n",
    "            n_enc = randint( 0 , n_encmax+1 )\n",
    "        else:\n",
    "            n_enc = n_encmax\n",
    "\n",
    "        LR  = choice(np.array([1,5]))*choice(np.array([0.00001 , 0.0001 , 0.001 , 0.01 , 0.1]))\n",
    "        \n",
    "        outLenc = np.sort(choice(np.arange(MINenc*5 , MAXenc*5+5 , 5 , dtype = int)  , n_enc))[::-1]\n",
    "        outLdec = np.sort(choice(np.arange(MINdec*5 , MAXdec*5+5 , 5 , dtype = int)  , n_dec))\n",
    "        \n",
    "        dropouts = choice([0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9] , n_enc+1)\n",
    "\n",
    "        l1_reg = choice([1,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10])\n",
    "\n",
    "        initials = choice(np.array(initializations))\n",
    "        inits.append(initials)\n",
    "        \n",
    "        act = choice(activations)\n",
    "        ACT.append(act)\n",
    "        DROPS.append(dropouts)\n",
    "\n",
    "        # Generating the model with the randomly generated hyperparameters\n",
    "        \n",
    "        model = Net(ENCSPDIM , in_channels , n_enc , n_dec , outLenc , outLdec , act , dropouts , True).to(device)\n",
    "        #print(model)\n",
    "        weight = choice([1,1e-1,1e-2,1e-3,1e-4,1e-5,1e-6,1e-7,1e-8,1e-9,1e-10])\n",
    "\n",
    "        weight_init(model, init_method = initials)\n",
    "        \n",
    "        optimizer = choice([optim.Adam(model.parameters() , lr = LR , weight_decay = weight)])\n",
    "\n",
    "        for epoch in range(try_epochs):\n",
    "            model.train()\n",
    "            LOSS = 0\n",
    "            for x in TL:\n",
    "                x = x.float().to(device)\n",
    "                y = model(x)\n",
    "                loss = criterion(y, x)\n",
    "                l1_lambda = l1_reg\n",
    "                l1 = sum(param.abs().sum() for param in model.parameters())\n",
    "                loss += l1_lambda * l1\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                LOSS += loss.item()\n",
    "            LOSS /= len(TL)\n",
    "            losses.append(LOSS)\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        VAL_LOSS = 0\n",
    "        with torch.no_grad():\n",
    "            for x in VL:\n",
    "                x = x.float().to(device)\n",
    "                y = model(x)\n",
    "                val_loss = criterion(y, x)\n",
    "                VAL_LOSS += val_loss.item()\n",
    "        VAL_LOSS /= len(VL)\n",
    "        val_l.append(VAL_LOSS)\n",
    "\n",
    "        # Updating the arrays with the randomly generated values\n",
    "        rate.append(LR)\n",
    "        W.append(weight)\n",
    "        enc.append(ENCSPDIM)\n",
    "        L1.append(l1_reg)\n",
    "\n",
    "        for j in range(n_enc):\n",
    "            Lenc[i,j] = outLenc[j]\n",
    "        for j in range(n_dec):\n",
    "            Ldec[i,j] = outLdec[j]\n",
    "        \n",
    "        # Updating the arrays of the losses + accuracy with the last losses\n",
    "        train_l.append(np.array(losses)[-1])#.mean())\n",
    "        print('Final loss value = ' , train_l[i])\n",
    "\n",
    "    J = np.argmin(np.asarray(val_l))#train_l))\n",
    "    \n",
    "    # Printing out the best values relatively to the chosen method\n",
    "    print( '\\x1b[4;34;43m'+'Best results is '+'\\x1b[0m' , J , '\\x1b[4;34;43m'+'. With values: '+'\\x1b[0m' ,\n",
    "           \"\\n\" ,  hyperparanames , \"\\n\" , rate[J] , Lenc[J] , Ldec[J] , W[J] , enc[J] , ACT[J] , DROPS[J] , inits[J] , L1[J] )#, seeds[J] )\n",
    "    \n",
    "    return rate[J] , Lenc[J] , Ldec[J] , W[J] , enc[J] , ACT[J] , DROPS[J] , inits[J] , L1[J]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21aefd5a-d665-4799-b47a-eb0476a4abec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per(lst):\n",
    "    return [list(p) for p in permutations(lst)]\n",
    "\n",
    "p_space = {\n",
    "    'num_epochs': [25],\n",
    "    'num_layers_encoder': [0,1],#,2,3,4,5],\n",
    "    'out1': get_per([5,10,15,20,25,30,35,40,45]),#get_per(np.arange(10 , 330 , 10 , dtype = int)),\n",
    "    'num_layers_decoder': [0,1],#,2,3,4,5],\n",
    "    'out2': get_per([5,10,15,20,25,30,35,40,45]),#get_per(np.arange(10 , 330 , 10 , dtype = int)),\n",
    "    'act': [nn.ReLU() , nn.ELU() , nn.SiLU() , nn.GELU() , nn.Mish() , nn.Sigmoid() , nn.Tanh()],\n",
    "    'weight_init': ['kaiming_normal', 'kaiming_uniform', 'xavier_normal', 'xavier_uniform'],\n",
    "    'learning_rate': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "    'weight_decay': [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8],\n",
    "    'batch_size': [spectra.shape[0]]# , 1024 , 512 , 256 , 128 , 64]\n",
    "}\n",
    "\n",
    "#print(p_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "6a12d0ce-7858-4459-a101-eb55290e2da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744, 781, 277)\n"
     ]
    }
   ],
   "source": [
    "shape = spectra2.shape#len(wavelength[a:b])#60\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ec558335-9fcd-4262-88de-60b5006e598a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "238118\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)\n",
    "DIM = 3\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_ds , test_ds = train_test_split(spectra , test_size=0.1, random_state=311996)\n",
    "\n",
    "train_ds , valid_ds = train_test_split(train_ds , test_size=0.33, random_state=200960)\n",
    "\n",
    "print(len(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a2ae3c-def9-4022-bf0d-7a9989f9c201",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Try 0\n",
      "Final loss value =  5.63027206226252e-05\n",
      "Try 1\n",
      "Final loss value =  0.020442016422748566\n",
      "Try 2\n"
     ]
    }
   ],
   "source": [
    "a = find_nearest(wavelength , 1000)\n",
    "b = find_nearest(wavelength , 2600)\n",
    "\n",
    "shape = 277  # 46\n",
    "torch.manual_seed(533733874069400)\n",
    "params = RandomSearch_autoencoder( spectra , shape , nn.HuberLoss() ,\n",
    "                                   encoded_space_dim = np.arange(5,11).tolist() ,\n",
    "                                   n_encmax = 1 , n_decmax = 2 ,\n",
    "                                   MINenc = 2 , MAXenc = 15 ,\n",
    "                                   MINdec = 2 , MAXdec = 15 ,\n",
    "                                   activations = [ nn.Mish() , nn.GELU() , nn.SiLU() , nn.ReLU() ] ,\n",
    "                                   fix_enc = False , fix_dec = True , \n",
    "                                   try_epochs = 20 , N_try = 60, Seed = torch.seed() , \n",
    "                                   printer = 'off' ,\n",
    "                                   bs = spectra.shape[0] ,\n",
    "                                   num_pieces = spectra.shape[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e51094ee-7a83-4f3b-94c4-d0b36b5532f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=277, out_features=6, bias=True)\n",
      "    (1): Mish(inplace=True)\n",
      "    (2): GumbelSoftmax()\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=35, bias=True)\n",
      "    (1): Mish(inplace=True)\n",
      "    (2): Linear(in_features=35, out_features=50, bias=True)\n",
      "    (3): Mish(inplace=True)\n",
      "    (4): Linear(in_features=50, out_features=55, bias=True)\n",
      "    (5): Mish(inplace=True)\n",
      "    (6): Linear(in_features=55, out_features=60, bias=True)\n",
      "    (7): Mish(inplace=True)\n",
      "    (8): Linear(in_features=60, out_features=60, bias=True)\n",
      "    (9): Mish(inplace=True)\n",
      "    (10): Linear(in_features=60, out_features=277, bias=True)\n",
      "    (11): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "wav = len(wavelength[a:b])#60\n",
    "\n",
    "DIM = params[4]#DIM\n",
    "torch.manual_seed(533733874069400)#params[6])#205653840223300)\n",
    "model = Net( encoded_space_dim = DIM , \n",
    "            in_channels = shape ,\n",
    "             n_layers_encoder = np.count_nonzero(params[1]) ,\n",
    "             n_layers_decoder = np.count_nonzero(params[2]) ,\n",
    "             out1 = params[1].astype(int) , out2 = params[2].astype(int) ,\n",
    "             act = nn.Mish(x),#params[5] ,\n",
    "             drops = params[6] , last = True ).to(device)\n",
    "\n",
    "weight_init(model, init_method = params[-2])\n",
    "\n",
    "print(model)\n",
    " \n",
    "loader = torch.utils.data.DataLoader( dataset = spectra ,\n",
    "                                      batch_size = spectra.shape[0],\n",
    "                                      shuffle = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "62dcbc1e-8266-4696-a03d-50d181fdde32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 2/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 3/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 4/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 5/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 6/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 7/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 8/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 9/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 10/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 11/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 12/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 13/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n",
      "Epoch 14/500 -> Train Loss: 0.0000 | Validation Loss: 0.0000 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[107], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train_losses, validation_losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_cnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHuberLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                             \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                             \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprinter\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[96], line 99\u001b[0m, in \u001b[0;36mtrain_cnn\u001b[1;34m(model, criterion, train_ds, validation_ds, num_epochs, patience, weight_decay, bs, device, LR, printer)\u001b[0m\n\u001b[0;32m     97\u001b[0m y_true \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     98\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m validation_loader:\n\u001b[0;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    101\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(x)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\clean_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:285\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    283\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_losses, validation_losses = train_cnn( model, nn.HuberLoss(), \n",
    "                                             train_ds, valid_ds, \n",
    "                                             500 , 25,\n",
    "                                             params[-1], len(train_ds), \n",
    "                                             DEVICE, params[0], printer = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "46f3ea3f-135f-4a97-b5b5-1b509d67f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"../../Desktop/FRT00009b5a_model_weights_spectral_parameter.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cad0fea-5eaf-45ff-a783-4221508eaba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.plot(train_losses , 'b' , linewidth = 1 , label = 'train loss')\n",
    "plt.plot(validation_losses , 'r' , linewidth = 1 , label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Extract Low-dimensional Representations\n",
    "encoded_data = model.encoder(torch.tensor(spectra).float().to(device)).detach().cpu().numpy()#model.encoder(torch.tensor(spectra).float()).detach().numpy()\n",
    "\n",
    "def plot_n_encoded(spectra , model , DIM , plot = True):\n",
    "    # Extract Low-dimensional Representations\n",
    "    encoded_data = model.encoder(torch.tensor(spectra).float()).detach().numpy()\n",
    "    \n",
    "    if plot:\n",
    "        fig , ax = plt.subplots(DIM,DIM)\n",
    "        for i in range(DIM):\n",
    "            for j in range(DIM):\n",
    "                if i != j and i < j:\n",
    "                    ax[i,j].plot(encoded_data[:,i] , encoded_data[:,j] , 'k.')\n",
    "                    ax[i,j].set_xlabel('Encoded Dimension '+str(i+1) , fontsize = 5)\n",
    "                    ax[i,j].set_ylabel('Encoded Dimension '+str(j+1) , fontsize = 5)\n",
    "                else:\n",
    "                    ax[i,j].axis('off')\n",
    "        plt.show()\n",
    "    return encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a451a90-f0b7-4e70-b62e-91f63027af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(744, 781, 60)\n",
      "['R770' 'RBR' 'BD530_2' 'SH600_2' 'SH770' 'BD640_2' 'BD860_2' 'BD920_2'\n",
      " 'RPEAK1' 'BDI1000VIS' 'R440' 'IRR1' 'BDI1000IR' 'OLINDEX3' 'R1330'\n",
      " 'BD1300' 'LCPINDEX2' 'HCPINDEX2' 'VAR' 'ISLOPE1' 'BD1400' 'BD1435'\n",
      " 'BD1500_2' 'ICER1_2' 'BD1750_2' 'BD1900_2' 'BD1900R2' 'BDI2000'\n",
      " 'BD2100_2' 'BD2165' 'BD2190' 'MIN2200' 'BD2210_2' 'D2200' 'BD2230'\n",
      " 'BD2250' 'MIN2250' 'BD2265' 'BD2290' 'D2300' 'BD2355' 'SINDEX2' 'ICER2_2'\n",
      " 'MIN2295_2480' 'MIN2345_2537' 'BD2500_2' 'BD3000' 'BD3100' 'BD3200'\n",
      " 'BD3400_2' 'CINDEX2' 'BD2600' 'IRR2' 'IRR3' 'R530' 'R600' 'R1080' 'R1506'\n",
      " 'R2529' 'R3920']\n",
      "(744, 781, 3)\n"
     ]
    }
   ],
   "source": [
    "print(rgbs.shape)\n",
    "print(sr_names)\n",
    "i = np.where((np.array(sr_names) == 'R2529'))[0][0]\n",
    "j = np.where((np.array(sr_names) == 'R1506'))[0][0]\n",
    "k = np.where((np.array(sr_names) == 'R1080'))[0][0]\n",
    "print(rgbs[:,:,[i,j,k]].shape)\n",
    "\n",
    "sr_names = np.array(sr_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b8fd0d8d-c60b-408e-8754-4b41bd9784e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 (394890, 10)\n",
      "58 57 56\n"
     ]
    }
   ],
   "source": [
    "DIM = 10\n",
    "RGB_autoencoder = np.zeros( (img_sr.shape[0] , img_sr.shape[1] , DIM) )\n",
    "\n",
    "print(DIM , encoded_data.shape)\n",
    "\n",
    "for j in range(DIM):\n",
    "    E = encoded_data[:,j]\n",
    "    for i in range(len(indexes[:,0])):\n",
    "        x , y = int(indexes[i,0]) , int(indexes[i,1])\n",
    "        RGB_autoencoder[x,y,j] = E[i]\n",
    "\n",
    "false = pyfresco.SpectraExtract(img, Nbands, wavelength, 400, 2600)\n",
    "fal = false.upload_map('FAL' , folder = 'FRT000062e6/')\n",
    "\n",
    "I = np.where((np.array(sr_names) == 'R2529'))[0][0]\n",
    "J = np.where((np.array(sr_names) == 'R1506'))[0][0]\n",
    "K = np.where((np.array(sr_names) == 'R1080'))[0][0]\n",
    "print(I , J , K)\n",
    "fig , ax = plt.subplots(1,DIM+1)\n",
    "ax[0].imshow( rgbs[:,:,[I,J,K]])\n",
    "ax[0].axis('off')\n",
    "ax[0].set_title('False color')\n",
    "for j in range(DIM):\n",
    "    ax[j+1].imshow( RGB_autoencoder[:,:,j] , cmap = 'viridis' )\n",
    "    ax[j+1].axis('off')\n",
    "    ax[j+1].set_title('Neuron '+str(j+1))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "cmap = matplotlib.colormaps.get_cmap('viridis')\n",
    "\n",
    "bounds = np.linspace(0,DIM,DIM+1 , dtype = int)\n",
    "norm = mpl.colors.BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "pixelsN = np.zeros((img.shape[0] , img.shape[1]))\n",
    "for i in range(DIM):\n",
    "    mappa = RGB_autoencoder[:,:,i]\n",
    "    ind = np.argwhere(RGB_autoencoder[:,:,i] != 0)\n",
    "    pixelsN[ind[:,0] , ind[:,1]] = i\n",
    "\n",
    "for i in range(img.shape[0]):\n",
    "    for j in range(img.shape[1]):\n",
    "        if img[i,j,0] == 65535:\n",
    "            pixelsN[i,j] = -1\n",
    "\n",
    "fig , ax = plt.subplots(1,2)\n",
    "ax[0].imshow( rgbs[ : , : , [I,J,K] ] )\n",
    "im = ax[1].imshow(pixelsN , cmap = cmap)\n",
    "plt.colorbar(im , ticks=bounds , norm = norm , boundaries=bounds)\n",
    "ax[0].axis('off')\n",
    "ax[1].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7239a9ff-5a94-4e59-8d57-b218057a3f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "n_plots = DIM + 1  # 1 for false color + DIM neurons\n",
    "n_cols = math.ceil(math.sqrt(n_plots))\n",
    "n_rows = math.ceil(n_plots / n_cols)\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 3*n_rows))\n",
    "axes = axes.flatten()  # Flatten to use as a 1D list\n",
    "\n",
    "# Plot false color image\n",
    "axes[0].imshow(rgbs[:, :, [I, J, K]])\n",
    "axes[0].axis('off')\n",
    "axes[0].set_title('False color')\n",
    "\n",
    "# Plot each neuron's activation map\n",
    "for j in range(DIM):\n",
    "    axes[j+1].imshow(RGB_autoencoder[:, :, j], cmap='viridis')\n",
    "    axes[j+1].axis('off')\n",
    "    axes[j+1].set_title('Neuron ' + str(j+1))\n",
    "\n",
    "# Hide unused axes if any\n",
    "for i in range(n_plots, len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "04e5edb7-e95a-4100-bbcf-86f4b45c4774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import StrMethodFormatter\n",
    "w1 , w2 = 400,2600\n",
    "A = find_nearest(wavelength , w1)\n",
    "B = find_nearest(wavelength , w2)\n",
    "wav = wavelength[A:B]\n",
    "\n",
    "means = []\n",
    "stds = []\n",
    "\n",
    "k , l = 0 , 0\n",
    "fig , ax = plt.subplots(2,DIM , figsize = [5,5] , sharey='row')\n",
    "for i in range(DIM):\n",
    "    \n",
    "    i_n = np.argwhere(RGB_autoencoder[:,:,i] == 1)\n",
    "\n",
    "    \n",
    "    S = np.array(img[:,:,:])[i_n[:,0],i_n[:,1],A:B]\n",
    "    \n",
    "    if 65535 in S:\n",
    "        S = np.delete(S , np.argwhere(S == np.ones(B-A , dtype = float)*65535.0) , axis = 0)\n",
    "    \n",
    "    s , se = np.mean(S , axis = 0) , np.std(S , axis = 0)\n",
    "    \n",
    "    means.append(s)\n",
    "    stds.append(se)\n",
    "    \n",
    "for i in range(DIM):\n",
    "    ax[0,i].plot(wav , means[i] , 'k')\n",
    "    ax[0,i].plot(wav , means[i]+stds[i] ,'k--')\n",
    "    ax[0,i].plot(wav , means[i]-stds[i] ,'k--')\n",
    "    ax[0,i].fill_between(wav , means[i]-stds[i] , means[i]+stds[i] , color = 'black' , alpha = 0.5)\n",
    "    ax[0,i].set_title('Neuron '+str(i+1))\n",
    "    \n",
    "    ax[1,i].imshow(RGB_autoencoder[:,:,i])\n",
    "    ax[1,i].set_xticks([])\n",
    "    ax[1,i].set_yticks([])\n",
    "    ax[0,i].set_xlabel('Wavelength [nm]')\n",
    "        \n",
    "ax[0,0].set_ylabel('Reflectance')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08bd1021-a85c-4281-a64a-01b591e33bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(1,DIM , sharey = True)\n",
    "for i in range(DIM):\n",
    "    ax[i].plot(wav , means[i]/means[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "857709d7-03f3-41fa-a3fa-a6b4e5f567ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "MEANS = np.zeros((DIM , len(means[0])))\n",
    "STDS = np.zeros((DIM , len(means[0])))\n",
    "for i in range(DIM):\n",
    "    MEANS[i,:] = means[i]\n",
    "    STDS[i,:] = stds[i]\n",
    "MEANS = np.nan_to_num(MEANS)\n",
    "STDS = np.nan_to_num(STDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "97309b1c-7f09-4805-b39d-e41c6745c26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rem_means = compute_removal(wav , MEANS , interp_type = 'linear')\n",
    "rem_stds = compute_removal(wav , STDS , interp_type = 'linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2ead926b-ddf8-401c-b642-f48c2fd63d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig , ax = plt.subplots(2,DIM , figsize = [5,5] , sharey='row')\n",
    "for i in range(DIM):\n",
    "    ax[0,i].plot(wav , rem_means[i] , 'k')\n",
    "    ax[0,i].set_title('Neuron '+str(i+1))\n",
    "    \n",
    "    ax[1,i].imshow(RGB_autoencoder[:,:,i])\n",
    "    ax[1,i].set_xticks([])\n",
    "    ax[1,i].set_yticks([])\n",
    "    ax[0,i].set_xlabel('Wavelength [nm]')\n",
    "        \n",
    "ax[0,0].set_ylabel('Reflectance')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
